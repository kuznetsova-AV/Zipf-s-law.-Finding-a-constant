1. **Импорт необходимых библиотек**

   В начале программы мы подключили модули, которые понадобятся для анализа текста и построения графика:

   ```python
   import re
   from collections import Counter
   import matplotlib.pyplot as plt
   ```

   - `re` — стандартная библиотека Python для работы с регулярными выражениями. Она позволяет корректно извлекать отдельные слова из текста, удаляя лишние символы.
   - `Counter` из модуля `collections` — удобный инструмент для подсчёта количества повторений каждого слова. Благодаря нему мы легко формируем частотный словарь.
   - `matplotlib.pyplot` — библиотека для визуализации данных. Используется для построения графика зависимости частоты от ранга (классическая Zipf-кривая).

2. **Создание функции для извлечения слов из текста**

   Далее мы описали функцию `get_words`, которая получает на вход строку (весь текст) и возвращает список слов:

   ```python
   def get_words(text: str):
       return re.findall(r'\w+', text.lower(), flags=re.UNICODE)
   ```

   Здесь:
   - текст переводится в нижний регистр (`text.lower()`), чтобы единообразно учитывать слова;
   - регулярное выражение `\w+` находит последовательности букв и цифр (включая русские);
   - флаг `re.UNICODE` разрешает корректное распознавание кириллицы.

   Таким образом мы получаем чистый список слов без знаков препинания.

3. **Описание основной функции анализа по закону Зипфа**

   Мы определили функцию `zipf_constant`, внутри которой выполняются основные расчёты:

   ```python
   def zipf_constant(filename: str, top_n: int | None = None, show_plot: bool = True):
   ```

   Она:
   - читает текст из файла,
   - формирует частотный словарь,
   - сортирует слова по убыванию частоты,
   - вычисляет Zipf-константу для каждого слова,
   - строит график.

4. **Чтение текста из файла**

   ```python
   with open(filename, 'r', encoding='utf-8') as f:
       text = f.read()
   ```

   На этом этапе мы открываем файл с текстом и считываем его полностью. Используется кодировка UTF-8, чтобы читать русские символы.

5. **Разделение текста на слова**

   ```python
   words = get_words(text)
   total_words = len(words)
   ```

   Здесь мы:
   - передаём текст в функцию `get_words`,
   - получаем список всех слов,
   - считаем их количество.

   Это необходимо для вычисления относительных частот.

6. **Подсчёт частот слов**

   ```python
   counter = Counter(words)
   ```

   С помощью `Counter` мы автоматически получаем, сколько раз каждое слово встречается в тексте. Это превращает список слов в частотный словарь.

7. **Сортировка слов по убыванию частоты**

   ```python
   sorted_words = counter.most_common()
   ```

   `most_common()` возвращает слова в порядке убывания частоты. Первый элемент — самое частое слово, второй — следующее и так далее.

8. **Ограничение на количество анализируемых слов**

   ```python
   if top_n is not None:
       sorted_words = sorted_words[:top_n]
   ```

   Если указан параметр `top_n`, мы анализируем только первые N слов, например 200. Это позволяет упростить график и расчёты.

9. **Подготовка списков для будущих вычислений**

   ```python
   constants = []
   ranks = []
   freqs = []
   ```

   Здесь мы создаём списки, куда будем сохранять:
   - `constants` — значения `C = f * r` для каждого слова;
   - `ranks` — ранги слов;
   - `freqs` — относительные частоты.

10. **Цикл по словам и вычисление Zipf-константы**

    ```python
    for rank, (word, freq) in enumerate(sorted_words, start=1):
    ```

    В этом цикле мы перебираем слова по порядку:
    - `rank` — текущий ранг слова;
    - `word` — слово;
    - `freq` — сколько раз оно встретилось в тексте.

    ```python
    C = freq * rank
    ```

    Здесь считается основная величина закона Зипфа: константа `C = f × r`.

    ```python
    constants.append(C)
    ranks.append(rank)
    freqs.append(freq / total_words)
    ```

    Мы сохраняем:
    - найденную константу,
    - ранг,
    - относительную частоту слова (для построения графика).

11. **Вывод первых 20 слов в консоль**

    ```python
    if rank <= 20: 
        print(f"{rank}\t{word}\t{freq}\t{C}")
    ```

    Чтобы не перегружать вывод, мы показываем только первые 20 самых частых слов и их значения `f`, `r` и `C`.

12. **Вычисление средней константы по выборке**

    ```python
    avg_C = sum(constants) / len(constants)
    print(f"\nОценка константы C по {len(constants)} словам: {avg_C:.2f}")
    ```

    Считаем среднее значение `C`. В теории оно должно быть примерно постоянным — это и есть проверка закона Зипфа.

Далее проанализируем результат выполнения кода программы.

<img width="672" height="958" alt="image" src="https://github.com/user-attachments/assets/e8bbffdb-60a1-4a46-b107-5349f27bcb18" />



В тексте содержится 1606 слов, включая повторы.

Самые частые слова в тексте — это служебные части речи (предлоги, союзы, частицы) и некоторые существительные. Это типично для естественного языка, где служебные слова часто встречаются чаще всего.

Например, слово "и" встречается 95 раз, что делает его самым частым словом в тексте.

**Константа Зипфа (C = f * r):**

Для каждого слова рассчитывается произведение его частоты (`f`) на его ранг (`r`). Это произведение должно быть примерно постоянным для всех слов, если текст подчиняется закону Зипфа.

В приведенном выводе видно, что значения `C` для первых 20 слов варьируются от 95 до 200. Это указывает на то, что закон Зипфа выполняется не идеально, но в целом тенденция сохраняется.

**Средняя константа C по 200 словам составляет 262.46.** Это значение можно использовать для оценки того, насколько хорошо текст подчиняется закону Зипфа.
<img width="974" height="731" alt="image" src="https://github.com/user-attachments/assets/fc22feb0-0cf0-4bec-a44c-c134c67e167e" />


**График** показывает зависимость частоты встречаемости слова от его ранга. Если текст подчиняется закону Зипфа, то график должен быть примерно линейным в логарифмическом масштабе. На графике видно, что частота слов быстро уменьшается с увеличением ранга, что соответствует закону Зипфа.

В целом, результаты показывают, что текст в целом подчиняется закону Зипфа, хотя и с некоторыми отклонениями.
