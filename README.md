# Анализ закона Зипфа для корпуса дипломных текстов

Данный проект реализует программный анализ текстов на основе закона Зипфа: выполняет токенизацию, очистку текста, расчёт параметров распределения, визуализацию экспериментальной и теоретической Zipf-кривой и сравнительный анализ дипломных работ.

---

## Токенизация и очистка текста

В проекте используется функция `get_words()`, которая выполняет последовательную предобработку текста:

1. выделяет слова с помощью регулярного выражения `\w+`;
2. приводит все токены к нижнему регистру;
3. удаляет:
   - базовые русские стоп-слова (предлоги, союзы, частицы, местоимения);
   - английские служебные слова;
   - **индивидуальные стоп-слова**, уникальные для каждого диплома (технические термины, единицы измерения, фрагменты кода, названия организаций);
   - чистые числа;
   - односимвольные токены;
   - короткие латинские технические аббревиатуры длиной 2–3 символа.

Целью является получение набора **лексически значимых слов**, которые действительно участвуют в формировании статистического распределения текста.

---

## Расчёт параметров закона Зипфа

Функция `analyze_text()` выполняет основной математический анализ:

1. вычисляет число всех слов и уникальных токенов;
2. строит ранжированный список слов по убыванию частоты;
3. рассчитывает относительную частоту `f(r) = freq / total`;
4. вычисляет:
   - **среднюю константу** закона Зипфа:  
     ⟨F_r · r⟩ = среднее значение произведения частоты и ранга;
   - **оптимальную константу C\***, найденную методом наименьших квадратов:  
     C\* = (Σ f_exp(r)/r) / (Σ 1/r²);
5. формирует теоретическую кривую `f_theor(r) = C* / r`;
6. рассчитывает **MSE** — среднеквадратичное отклонение между экспериментальными и теоретическими частотами.

Анализ даёт численную оценку того, насколько текст соответствует закону Зипфа.

---

## Визуализация результатов

Функция `plot_zipf()` строит два графика в логарифмических координатах:

- экспериментальную кривую распределения слов;
- теоретическую Zipf-кривую формата `C/r`.

Используется логарифмическое масштабирование обеих осей (`log–log`), что позволяет визуально сравнить два распределения и оценить степень их совпадения.

---

## Анализ всего корпуса текстов

Функция `analyze_corpus()`:

1. перебирает все `.txt` файлы в папке `corpus`;
2. для каждого файла:
   - выполняет безопасное чтение текста с поддержкой нескольких кодировок;
   - выполняет токенизацию текста и очистку;
   - вычисляет параметры закона Зипфа;
   - выводит статистический отчёт (число слов, уникальных токенов, C_mean, C_opt, MSE);
   - показывает 10 самых частотных слов;
3. сохраняет результаты анализа для дальнейших сравнений.

Этот модуль позволяет изучить, как по-разному устроены дипломные тексты коллектива.

---

## Сравнение двух дипломов

Функция `compare_two_files()` позволяет:

1. загрузить два выбранных текста из корпуса;
2. выполнить анализ каждого текста по закону Зипфа;
3. вывести их параметры:
   - среднюю константу ⟨F_r r⟩;
   - оптимальную константу C\*;
   - среднеквадратичное отклонение MSE;
4. построить две отдельные Zipf-кривые.

Это предоставляет возможность проводить сравнительный лингвистический анализ.

---

## Почему в проекте используются индивидуальные стоп-слова

Разные дипломы имеют различную природу:

- программный код (Fortran, GLSL);
- математические переменные;
- единицы измерения (м, кг, МПа);
- сокращения технической документации;
- названия организаций (ПАО, ООО, Газпром, Сургутнефтегаз);
- индексные и служебные переменные (l1, tmp, irow).

Эти слова:

- часто встречаются в тексте;
- не несут смысловой нагрузки для анализа естественного языка;
- искажают частотную кривую;
- размазывают ранги;
- смещают константу закона Зипфа.

Поэтому каждому файлу соответствует **персональный набор стоп-слов**, который позволяет:

- корректно очистить текст,
- удалить шумовые элементы,
- анализировать реальную лексику,
- получить достоверные результаты.

---

## Используемые методы

В проекте применяются:

- регулярные выражения для токенизации текста;
- статистическая обработка частотных распределений;
- ранжирование по убыванию частоты;
- расчёт основной и оптимальной констант закона Зипфа;
- метод наименьших квадратов для аппроксимации C\*;
- построение графиков в логарифмических координатах (`matplotlib`);
- очистка текста через фильтрацию стоп-слов и шумовых токенов.

---

## Итог

Проект реализует полный цикл анализа текста по закону Зипфа:

- чтение и предобработка текста;
- построение частотного распределения;
- вычисление констант и оценка качества аппроксимации;
- визуальная проверка соответствия распределению `C/r`;
- сравнение текстов между собой;
- исследование стилистических различий и структурной организации дипломных работ.

Полученный функционал может использоваться для:

- корпусной лингвистики,
- статистического анализа русскоязычных текстов,
- сравнительных исследований,
- подготовки отчётных материалов и научных работ.
